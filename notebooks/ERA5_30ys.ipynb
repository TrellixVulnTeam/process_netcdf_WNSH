{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests import HTTPError\n",
    "from typing import List, Optional\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "site = 'http://37.128.186.209/LAURA/ERA5/30year'\n",
    "def get_html_data_list(site: str) -> List[str]:\n",
    "    result = []\n",
    "    html = requests.get(site)\n",
    "    try:\n",
    "        html.raise_for_status()\n",
    "    except HTTPError as err:\n",
    "        print(f'cannot get html from {site} because of {html.status_code}')\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    for link in soup.find_all('a'):\n",
    "        if link.get('href').startswith('ERA5'):\n",
    "            result.append(link.get('href'))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "html = get_html_data_list('http://37.128.186.209/LAURA/ERA5/30year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_url(base_url: str, filename: str) -> str:\n",
    "    stripped_url = base_url.rstrip('/')\n",
    "    return f'{stripped_url}/{filename}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def make_dest_path(filename: str) -> str:\n",
    "    cwd = Path.cwd()\n",
    "    temp = cwd / 'temp'\n",
    "    temp.mkdir(exist_ok=True)\n",
    "    return f'{str(temp)}/{filename}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "def download_file(site: str, dest_path: str) -> Optional[str]:\n",
    "    with requests.get(site, stream=True) as r:\n",
    "        try:\n",
    "            r.raise_for_status()\n",
    "        except HTTPError as err:\n",
    "            print(f'cannot download file from {site} because of {r.status_code}')\n",
    "        try:\n",
    "            if path.exists(dest_path):\n",
    "                print(f'file {dest_path} already downloaded')\n",
    "                return dest_path\n",
    "            with open(dest_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "        except Exception as e:\n",
    "            print(f'cannot download file because of {e}')\n",
    "            dest_path = None\n",
    "    return dest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filename = download_file('http://37.128.186.209/LAURA/ERA5/30year/ERA5_30year_2mTemp.tar.gz','ERA5_30year_2mTemp.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_state_file (filetype: str) -> str:\n",
    "    cwd = Path.cwd()\n",
    "    temp = cwd / 'temp'\n",
    "    temp.mkdir(exist_ok=True)\n",
    "    return f'{str(temp)}/state_{filetype}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def insert_into_state_file(state_file_path : str, filename: str, status: str) -> None:\n",
    "    with open(state_file_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        if downloaded_file is not None:\n",
    "            writer.writerow([f'{filename}', f'{status}'])\n",
    "    print(f'inserted status {status} for file {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_unpacked_folder(filename: str) -> str:\n",
    "    filename = filename.rstrip('.tar.gz')\n",
    "    cwd = Path.cwd()\n",
    "    temp = cwd / 'unpacked_temp' / filename\n",
    "    temp.mkdir(mode=0o777, parents= True, exist_ok=True)\n",
    "    return str(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def unpack_file(compressed_file: str, destination: str) -> None:\n",
    "    if len(os.listdir(destination))>0 :\n",
    "        print(f'file {compressed_file} already unpacked in destination {destination}')\n",
    "        return\n",
    "    with tarfile.open(compressed_file, mode='r:gz') as tar:\n",
    "        tar.extractall(path=destination)\n",
    "#     write state unpacked\n",
    "    print (f'file extracted in {destination}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_child_unpacked_folder(unpacked_folder: str) -> str:\n",
    "    list_dir = os.listdir(unpacked_folder)\n",
    "    if len(list_dir) != 1:\n",
    "        raise ValueError(f'{unpacked_folder} contains more than a child or is empty')\n",
    "    return unpacked_folder + '/' + list_dir[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "def get_zipped_nc_file(folder: str) -> Generator[Path, None, None]:\n",
    "    return Path(folder).glob('*.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "def unzip_nc_file(filepath: str) -> str:\n",
    "    with gzip.open(filepath, 'rb') as f:\n",
    "        output_filepath = filepath.rstrip('.gz')\n",
    "        with open(output_filepath, 'wb') as w:\n",
    "            while True:\n",
    "                piece = f.read(1024)\n",
    "                if not piece:\n",
    "                    break\n",
    "                w.write(piece)\n",
    "    return output_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def delete_compressed_nc_file(filepath: str) -> Optional[str]:\n",
    "    if filepath.endswith('.gz'):\n",
    "        os.remove(filepath)\n",
    "        return filepath\n",
    "    else:\n",
    "        raise ValueError(f'file; {filepath} is not compressed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def wait_for_decompress(filepath: str) -> None:\n",
    "    while True:\n",
    "        if len(list(Path(filepath).glob('*.gz'))) == 0:\n",
    "            break\n",
    "        print (f'there are still netcdf files to decompress')\n",
    "        sleep(1)\n",
    "    print(f'decompressed all netcdfs ------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from iris.experimental.equalise_cubes import equalise_attributes\n",
    "\n",
    "def merge_cubes(cube_list):\n",
    "    equalise_attributes(cube_list)\n",
    "    return cube_list.merge_cube()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_output_nc_filepath(folder_with_nc: str) -> str:\n",
    "    cwd = Path.cwd()\n",
    "    temp = cwd / 'results'\n",
    "    temp.mkdir(mode=0o777, exist_ok=True)\n",
    "    base = folder_with_nc.rsplit('/', 1)[0]\n",
    "    filename = base.rsplit('/', 1)[-1] + '.nc'\n",
    "    return str(temp) + '/' + filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def merge_all_nc(filepath: str, output_path:str) -> str:\n",
    "    nc_gen = Path(filepath).glob('*.nc')\n",
    "    cube_list = []\n",
    "    for nc in nc_gen:\n",
    "        cube_to_merge = iris.load_cube(str(nc))\n",
    "        cube_list.append(cube_to_merge)\n",
    "    merged_cube = merge_cubes(iris.cube.CubeList(cube_list))\n",
    "    iris.save(merged_cube, output_path)\n",
    "    print (f'saved merged nc file in {output_path}')\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from shutil import rmtree\n",
    "def cleanup_folder(folder_path: str)->None:\n",
    "    rmtree(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "def get_aws_config() -> Tuple[str, str, str, str]:\n",
    "    endpoint_url = os.getenv('S3_URL','http://s3-uk-1.sa-catapult.co.uk')\n",
    "    access_key = os.getenv('S3_ID', '')\n",
    "    secreat_access_key = os.getenv('S3_KEY', '')\n",
    "    s3_bucket = os.getenv('S3_BUCKET', '')\n",
    "    return endpoint_url, access_key, secreat_access_key, s3_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "from time import sleep\n",
    "\n",
    "def upload_to_s3(src_path: str, dest_path: str) -> None:\n",
    "    endpoint_url, access_key, secreat_access_key, s3_bucket = get_aws_config()\n",
    "    print (f'starting uploading {src_path}')\n",
    "    s3 = boto3.client('s3', endpoint_url=endpoint_url, aws_access_key_id=access_key,\n",
    "                      aws_secret_access_key=secreat_access_key)\n",
    "    s3.upload_file(src_path, s3_bucket, dest_path)\n",
    "    print (f'Uploaded {src_path} to S3 as {dest_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def uncompress_downloaded_tar(downloaded_file_path: str, filename: str) -> str:\n",
    "    dest_unpacked = make_unpacked_folder(filename=filename)\n",
    "    unpack_file(compressed_file=downloaded_file_path, destination=dest_unpacked)\n",
    "    child_unpacked = get_child_unpacked_folder(unpacked_folder=dest_unpacked)\n",
    "    for nc_file in get_zipped_nc_file(child_unpacked):\n",
    "        _ = unzip_nc_file(str(nc_file))\n",
    "        _ = delete_compressed_nc_file(str(nc_file))\n",
    "    print(f'nc files unzipped in folder {dest_unpacked}')\n",
    "    return child_unpacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_s3_destination_filename(url: str, merged_nc_filepath: str) -> str:\n",
    "    data_recurrence = url.rsplit('/', 1)[-1]\n",
    "    merged_nc_filename = merged_nc_filepath.rsplit('/', 1)[-1]\n",
    "    return data_recurrence + '/' + merged_nc_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def check_if_file_exsists_in_results(file_path: str) -> bool:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "std_names = {\n",
    "    '2 metre temperature': 'surface_temperature',\n",
    "    'Total precipitation': 'precipitation_flux',\n",
    "    'Sea surface temperature': 'sea_surface_temperature',\n",
    "    'Soil temperature level 1': 'soil_temperature',\n",
    "    'Volumetric soil water layer 1': 'volume_fraction_of_condensed_water_in_soil'\n",
    "}\n",
    "def add_standard_name_to_cube(path_to_file: str, std_name_dict: dict) -> str:\n",
    "    original_cube = iris.load_cube(path_to_file)\n",
    "    print(original_cube)\n",
    "    if original_cube.standard_name is not None:\n",
    "        return path_to_file\n",
    "    std_name = std_name_dict[original_cube.long_name]\n",
    "    temp_out = path_to_file.rsplit('/', 1)[0] + '/' + std_name + '.nc'\n",
    "    print(temp_out)\n",
    "    original_cube.standard_name=std_name\n",
    "    iris.save(original_cube, temp_out)\n",
    "    Path(path_to_file).unlink()\n",
    "    Path(temp_out).rename(path_to_file)\n",
    "    print(f'added std name {std_name} to {path_to_file}')\n",
    "    return path_to_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import tarfile\n",
    "def main(url: str) -> None:\n",
    "    data_list = get_html_data_list(site=url)\n",
    "    print (f'there are {len(data_list)} file to process')\n",
    "    count = 0\n",
    "    for data in data_list:\n",
    "        count += 1\n",
    "        site = make_url(url, data)\n",
    "        dest_path = make_dest_path(filename=data)\n",
    "        print(f'start downloading {data}')\n",
    "        downloaded_file = download_file(site=site, dest_path=dest_path)\n",
    "        print (f'downloaded file {downloaded_file}')\n",
    "        folder_with_nc = uncompress_downloaded_tar(downloaded_file_path=downloaded_file, filename=data)\n",
    "        wait_for_decompress(folder_with_nc)\n",
    "        merged_nc_filepath = make_output_nc_filepath(folder_with_nc=folder_with_nc)\n",
    "        merge_all_nc(filepath=folder_with_nc,output_path=merged_nc_filepath)\n",
    "        add_standard_name_to_cube(path_to_file=merged_nc_filepath, std_name_dict=std_names)\n",
    "        cleanup_folder(folder_path=str(Path(folder_with_nc).parent))\n",
    "        print(f'processed {count} of {len(data_list)}')\n",
    "        s3_destination = make_s3_destination_filename(url=url, merged_nc_filepath=merged_nc_filepath)\n",
    "        print(f'S3 destination: {s3_destination}')\n",
    "        upload_to_s3(src_path=merged_nc_filepath, dest_path=s3_destination)\n",
    "        print(f'uploaded {count} files of {len(data_list)} | file uploaded to destination {s3_destination}')\n",
    "        Path(downloaded_file).unlink()\n",
    "        Path(merged_nc_filepath).unlink()\n",
    "    cleanup_folder(str(Path.cwd() / 'temp'))\n",
    "    cleanup_folder(str(Path.cwd() / 'unpacked_temp'))\n",
    "    cleanup_folder(str(Path.cwd() / 'results'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "main(url='http://37.128.186.209/LAURA/ERA5/30year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import iris\n",
    "from pathlib import Path\n",
    "cube = iris.load_cube(str(Path.cwd() / 'results' / 'ERA5_30year_2mTemp_std_name.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(cube.standard_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import iris\n",
    "from pathlib import Path\n",
    "cube = iris.load_cube(str(Path.cwd() / 'results' / 'ERA5_30year_2mTemp.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fn = str(Path.cwd() / 'results' / 'ERA5_30year_2mTemp.nc')\n",
    "add_standard_name_to_cube(path_to_file=fn, std_name_dict=std_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "file = 'ERA5_30year_2mTemp.tar.gz'\n",
    "state_file = make_state_file('yearly')\n",
    "site = make_url('http://37.128.186.209/LAURA/ERA5/30year', 'ERA5_30year_2mTemp.tar.gz')\n",
    "dest_path = make_dest_path(filename=file)\n",
    "downloaded_file = download_file(site=site, dest_path=dest_path)\n",
    "print (f'downloaded file {downloaded_file}')\n",
    "dest_unpacked = make_unpacked_folder(filename=file)\n",
    "print(f'dest unpacked: {dest_unpacked}')\n",
    "unpack_file(compressed_file=downloaded_file, destination=dest_unpacked)\n",
    "child_unpacked = get_child_unpacked_folder(unpacked_folder=dest_unpacked)\n",
    "print (child_unpacked)\n",
    "for nc_file in get_zipped_nc_file(child_unpacked):\n",
    "    output_filepath = unzip_nc_file(str(nc_file))\n",
    "    _ = delete_compressed_nc_file(str(nc_file))\n",
    "wait_for_decompress(child_unpacked)\n",
    "output_nc_path = make_output_nc_filepath(base_path=dest_unpacked, file_name=dest_unpacked.rsplit('/',1)[-1]+'.nc')\n",
    "saved_file = merge_all_nc(child_unpacked,output_path=output_nc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dest_unpacked.rsplit('/',1)[-1]+'.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p = Path('/home/jovyan/work/unpacked_temp/ERA5_30year_2mTemp/2mTemp/')\n",
    "[f for f in p.iterdir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "merge_all_nc(filepath='/home/jovyan/work/unpacked_temp/ERA5_30year_2mTemp/2mTemp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cube_1 = iris.load_cube('/home/jovyan/work/unpacked_temp/ERA5_30year_2mTemp/2mTemp/2mTemp_ERA5_SouthPacific_30_year_av_0101.nc')\n",
    "cube_2 = iris.load_cube('/home/jovyan/work/unpacked_temp/ERA5_30year_2mTemp/2mTemp/2mTemp_ERA5_SouthPacific_30_year_av_0102.nc')\n",
    "cube_list = merge_2_cubes(cube_1=cube_1, cube_2=cube_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def concatenate_2_cubes(cube_1, cube_2):\n",
    "    print(cube_1.coords(dim_coords=False))\n",
    "    cube_1.add_dim_coord('time',0)\n",
    "    cube_2.add_dim_coord('time',0)\n",
    "    cube_list = iris.cube.CubeList([cube_1, cube_2])\n",
    "    return cube_list.concatenate_cube()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cube_1 = iris.load_cube('/home/jovyan/work/unpacked_temp/ERA5_30year_2mTemp/2mTemp/2mTemp_ERA5_SouthPacific_30_year_av_0101.nc')\n",
    "cube_2 = iris.load_cube('/home/jovyan/work/unpacked_temp/ERA5_30year_2mTemp/2mTemp/2mTemp_ERA5_SouthPacific_30_year_av_0102.nc')\n",
    "cube_list = concatenate_2_cubes(cube_1=cube_1, cube_2=cube_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(cube_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def merge_all_nc(filepath: str):\n",
    "    nc_gen = Path(filepath).glob('*.nc')\n",
    "    cube_list = []\n",
    "    for nc in nc_gen:\n",
    "        cube_to_merge = iris.load_cube(str(nc))\n",
    "        cube_list.append(cube_to_merge)\n",
    "    print(cube_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from iris.experimental.equalise_cubes import equalise_attributes\n",
    "equalise_attributes(cube_list)\n",
    "res = cube_list.merge_cube()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res.derived_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import iris\n",
    "\n",
    "cube = iris.load_cube('/home/jovyan/work/unpacked_temp/ERA5_30year_2mTemp/2mTemp/2mTemp_ERA5_SouthPacific_30_year_av_0317.nc')\n",
    "for coord in cube.coords():\n",
    "    print(coord.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_nc_files_in_folder(nc_folder: str) -> str:\n",
    "#     get list of files\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_list\n",
    "soup = BeautifulSoup(html.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    if link.get('href').startswith('ERA5'):\n",
    "        print(link.get('href'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
